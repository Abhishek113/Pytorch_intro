{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_Intro.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "gpPmMMQlvk6r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pytorch: Intro:\n",
        "\n",
        "### what is pytorch?\n",
        "\n",
        "-> [Pytorch](https://pytorch.org/) is an *Open Source* machine learning liabrary for python based on Tourch developed by Facebook. (see [Wiki definition](https://en.wikipedia.org/wiki/PyTorch)).\n",
        "\n",
        "It provides:\n",
        "\n",
        "1.   Tensor computition like Numpy which able to get run by GPU.\n",
        "\n",
        "2.   Deep Neural network build on tape-based autodiff i.e. automatic differentiation (also called as auto-grad for automatic gradients) system which enables you to write relations between tensors functionally and differentiate throgh them.\n",
        "\n",
        "### What is tensor?\n",
        "\n",
        "-> Tensor is nothing but the higher dimenssion matrix. But why they are called as tensor instead of matrix. The reason is that matrix represents high-dimentional arrays but tensor also represents the relation between the numbers it contains. For example [Cauchy stress tensor](https://en.wikipedia.org/wiki/Cauchy_stress_tensor) every point of which represents the state of stress at a point in deformed material.\n",
        "\n",
        "-> Lets first install Pytorch.\n",
        "-> Google collabe doesn't have pytorch defaulty installed, you need to install it.\n",
        "-> Most stable version os 1.0.0\n",
        "-> Run this command in cell : \"***!pip install torch==1.0.0*\""
      ]
    },
    {
      "metadata": {
        "id": "C3nHgbspuOan",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install torch==1.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n8JPlK-L_gzF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ndaa0VrDCeBH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Initializing Tensors\n",
        "\n",
        "This section introduces the different ways of initializing the tensor.\n",
        "\n",
        "Just similar to the numpy you can initialize tensor as follows:"
      ]
    },
    {
      "metadata": {
        "id": "yvyzPUch_jmK",
        "colab_type": "code",
        "outputId": "4b16ffe2-a595-4d93-976f-4d4a84e9d89a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "x = torch.ones(3,2) # 3 rows and 2 colomns\n",
        "print(x)\n",
        "x = torch.zeros(3,2)\n",
        "print(x)\n",
        "x = torch.rand(3,2)\n",
        "print(x)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]])\n",
            "tensor([[0.8094, 0.8136],\n",
            "        [0.1963, 0.5464],\n",
            "        [0.2261, 0.9688]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BAcvQ4HsDB2n",
        "colab_type": "code",
        "outputId": "ffe8ebb0-c9d4-42e1-c171-193e94f83247",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "x = torch.empty(3,2) # It will just create a space for the tnsor (3,2) without initializing it.\n",
        "print(x)\n",
        "\n",
        "# But you can fill the above empty tensor as follows:\n",
        "y = torch.zeros_like(x)\n",
        "print(y)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2.6699e-35, 0.0000e+00],\n",
            "        [0.0000e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 0.0000e+00]])\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FY6NErn4D-br",
        "colab_type": "code",
        "outputId": "aec8a4e1-946e-49b9-917a-821c2e60bc48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x = torch.linspace(0, 1, steps=5) # linspace statnds for linear space. x contains number from 0 to 1 inclusively and equali devided by steps = 5\n",
        "print(x)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QKIgijhRDx6D",
        "colab_type": "code",
        "outputId": "6434787c-5844-4fda-aeff-5187b197bf1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[1,2], [3,4], [5,6]]) # hardcoded initializartion with desired values\n",
        "print(x)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UhhkkVe6M6LM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reshaping"
      ]
    },
    {
      "metadata": {
        "id": "izW6kS0kM8hj",
        "colab_type": "code",
        "outputId": "bdb858c2-1cfa-448a-924b-5e5ee895e863",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "print(x)\n",
        "y = x.view(3, 2) # it reshapes x as (3,2) which previously has shape (2, 3) and stores it in y\n",
        "print(y)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n",
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ben0gpI9NQn4",
        "colab_type": "code",
        "outputId": "2e095274-674b-40f1-a7c7-e30083b49f08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "x = y.view(6, -1)\n",
        "print(x)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1],\n",
            "        [2],\n",
            "        [3],\n",
            "        [4],\n",
            "        [5],\n",
            "        [6]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "87B5j8W_Npfp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Simple pytorch operations"
      ]
    },
    {
      "metadata": {
        "id": "TB3mf3yxFA5y",
        "colab_type": "code",
        "outputId": "7b5113a6-1b1e-469f-b2d9-8db568c60b43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "x = torch.ones(3,2)\n",
        "y = torch.ones(3,2)\n",
        "\n",
        "z = x + y # It performs element wise addition and resulting in tensor having same shape i.e. 3,2\n",
        "print(z)\n",
        "\n",
        "z = x - y # It performs element wise subtraction\n",
        "print(z)\n",
        "\n",
        "z = x * y # It performs element wise multiplication\n",
        "print(z)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2., 2.],\n",
            "        [2., 2.],\n",
            "        [2., 2.]])\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9iD15kFqOgFb",
        "colab_type": "code",
        "outputId": "1e828088-81f1-44de-8515-30550c94fbd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "z = y.add(x) # As tensor variable acts as an object it has add function. It stores addition result in z and both x, y remain unchanged.\n",
        "\n",
        "print(z)\n",
        "print(y)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2., 2.],\n",
            "        [2., 2.],\n",
            "        [2., 2.]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MsEmFy3_O3Mj",
        "colab_type": "code",
        "outputId": "087f2e9b-825e-4972-cd6c-19ae1bce87fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "z = y.add_(x) # It stores the result of addition in both z and y\n",
        "\n",
        "print(z)\n",
        "print(y)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2., 2.],\n",
            "        [2., 2.],\n",
            "        [2., 2.]])\n",
            "tensor([[2., 2.],\n",
            "        [2., 2.],\n",
            "        [2., 2.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Rw6OOJIQYIFk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Numpy <> Pytorch"
      ]
    },
    {
      "metadata": {
        "id": "qQgSuAz8PHqW",
        "colab_type": "code",
        "outputId": "06df690b-76b4-4ce7-9c3d-3d9dfbced116",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "x_np = x.numpy() # converting Pytorch tensor to numpy tensor\n",
        "print(type(x_np), type(x))\n",
        "print(x_np)\n",
        "print(x)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> <class 'torch.Tensor'>\n",
            "[[1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]]\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-t6NFjOUaEXA",
        "colab_type": "code",
        "outputId": "db8783b4-48d0-4cca-a86a-7bfc0f0b2b40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "a = np.random.randn(3,2)\n",
        "a_pt = torch.from_numpy(a)\n",
        "print(type(a_pt), type(a))\n",
        "print(a_pt)\n",
        "print(a)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'> <class 'numpy.ndarray'>\n",
            "tensor([[-0.4486, -0.8917],\n",
            "        [ 1.2347,  1.1371],\n",
            "        [-0.1117, -0.9747]], dtype=torch.float64)\n",
            "[[-0.44856405 -0.89170255]\n",
            " [ 1.23473346  1.13712243]\n",
            " [-0.1116943  -0.97466147]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O-nb2FcxadtS",
        "colab_type": "code",
        "outputId": "0be6916a-00cc-424d-b0ba-c9ac15587ff8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "# Here when we try to add any value in a it is also get added in to a_pt. See the results\n",
        "\n",
        "np.add(a, 1, out=a)\n",
        "print(a)\n",
        "print(a_pt)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.55143595 0.10829745]\n",
            " [2.23473346 2.13712243]\n",
            " [0.8883057  0.02533853]]\n",
            "tensor([[0.5514, 0.1083],\n",
            "        [2.2347, 2.1371],\n",
            "        [0.8883, 0.0253]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rSTzlJoGDxwj",
        "colab_type": "code",
        "outputId": "d5a57ce8-d666-4144-acfa-f02951139891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# execution time of numpy\n",
        "%%time\n",
        "\n",
        "for i in range(100):\n",
        "  a = np.random.randn(100,100)\n",
        "  b = np.random.randn(100,100)\n",
        "  c = a+b"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 88.8 ms, sys: 1.8 ms, total: 90.6 ms\n",
            "Wall time: 95.1 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bxuwy6ZfEaIX",
        "colab_type": "code",
        "outputId": "83e132dc-e334-4e18-d49c-09d56bdaa0dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# execution time of pytorch \n",
        "%%time\n",
        "\n",
        "for i in range(100):\n",
        "  a = torch.randn([100,100])\n",
        "  b = torch.randn([100,100])\n",
        "  c = a+b\n",
        "  \n",
        "# Pytorch is 3x faster than numpy"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 19.7 ms, sys: 2.1 ms, total: 21.8 ms\n",
            "Wall time: 22.6 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p29Xmv7EOEyY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CUDA Support"
      ]
    },
    {
      "metadata": {
        "id": "7glV4hHxEw0J",
        "colab_type": "code",
        "outputId": "6c71f937-c386-4c8a-d9c5-47fa51602709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "# First check the number of CUDA supported devices\n",
        "print(torch.cuda.device_count())\n",
        "\n",
        "'''\n",
        "At first time it shows you zero count as there is no GPU device is connected.\n",
        "So to use it go to Edit -> Notebook settings -> Select GPU from drop down button.\n",
        "After that the notebooks gets reconnected to lab.\n",
        "So run all the cells again sequestially and then this cell.\n",
        "You can see the count as 1 as CUDA device is available now.\n",
        "'''"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAt first time it shows you zero count as there is no GPU device is connected.\\nSo to use it go to Edit -> Notebook settings -> Select GPU from drop down button.\\nAfter that the notebooks gets reconnected to lab.\\nSo run all the cells again sequestially and then this cell.\\nYou can see the count as 1 as CUDA device is available now.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "metadata": {
        "id": "Mm2qWzrGOdfi",
        "colab_type": "code",
        "outputId": "f162db55-085a-420f-a5e4-065115485464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Reference to the object within torch:  \", torch.cuda.device(0))\n",
        "print(\"\\nDevice name: \", torch.cuda.get_device_name(0))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reference to the object within torch:   <torch.cuda.device object at 0x7f94b4495588>\n",
            "\n",
            "Device name:  Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sWb4zTOQQuf8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cuda0 = torch.device('cuda:0') # get device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CQD_6uhfRXuf",
        "colab_type": "code",
        "outputId": "225e0500-f554-4e97-9ef6-1e7f4e0ee685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Here we have given an additional argument as device=cuda0. \n",
        "This indicates that it uses device cuda0 for initilizing a and b.\n",
        "'''\n",
        "\n",
        "a = torch.randn(3, 2, device=cuda0)\n",
        "b = torch.randn(3, 2, device=cuda0)\n",
        "\n",
        "c = a+b\n",
        "\n",
        "'''\n",
        "Eventually when we add a and b it gets performed on GPU device cuda0 and \n",
        "result c also get initialized on cuda0. \n",
        "You can see the result C and you will the device='cuda:0'.\n",
        "'''\n",
        "print(c,\"\\n\\n\")\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.8998, -1.3132],\n",
            "        [ 3.2904,  1.6229],\n",
            "        [ 0.9953,  1.2194]], device='cuda:0') \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HkkAJWGLSOLd",
        "colab_type": "code",
        "outputId": "d6655f68-2153-4b80-eb88-9315b0cb3521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# execution time of numpy\n",
        "%%time\n",
        "\n",
        "for i in range(10): \n",
        "  a = np.random.randn(10000,10000)\n",
        "  b = np.random.randn(10000,10000)\n",
        "  c = a+b"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 29s, sys: 547 ms, total: 1min 29s\n",
            "Wall time: 1min 29s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JOjAyapcY6sA",
        "colab_type": "code",
        "outputId": "539fb1ed-ac21-4671-ad4c-f9a8392a35b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# execution time of pytorch\n",
        "%%time\n",
        "\n",
        "for i in range(10): \n",
        "  a = torch.randn(10000,10000)\n",
        "  b = torch.randn(10000,10000)\n",
        "  c = a+b"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 18.8 s, sys: 117 ms, total: 18.9 s\n",
            "Wall time: 18.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uLwyvhEub18W",
        "colab_type": "code",
        "outputId": "f4bafd87-3571-442f-a439-a5b6b6d6e004",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# execution time of pytorch on GPU\n",
        "%%time\n",
        "\n",
        "for i in range(100): # make 10 iterations while you run this cell\n",
        "  a = torch.randn(10000,10000, device=cuda0)\n",
        "  b = torch.randn(10000,10000, device=cuda0)\n",
        "  c = a+b\n",
        " \n",
        "'''You can see pytorch on GPU is 1000x faster than CPU'''"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6.22 ms, sys: 19 ms, total: 25.3 ms\n",
            "Wall time: 24.6 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wo5q673gVdXZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Autograd\n",
        "\n",
        "Automatic computation of gradients."
      ]
    },
    {
      "metadata": {
        "id": "5eMYICo0UHwL",
        "colab_type": "code",
        "outputId": "b0ff5f30-06af-47a7-8e92-4aca2021d9a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "x = torch.ones(3, 2, requires_grad=True) # an additional argument spacifies that it requires to calculate gradient so, it will automatically calculate gradients as we can it see next\n",
        "print(x)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eNoL0CBrdAEA",
        "colab_type": "code",
        "outputId": "bf95b223-d93a-4f09-a98c-2d6f77f40d32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# Here y is in linear relation with x\n",
        "\n",
        "y = x + 5\n",
        "print(y)\n",
        "\n",
        "# as y in in linear relation with x, it will calculate derivative of y with respect to x (which itself requires gradient) spacified by grad_fn=<AddBackward0>. "
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[6., 6.],\n",
            "        [6., 6.],\n",
            "        [6., 6.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KmvVoGztdJnn",
        "colab_type": "code",
        "outputId": "b8215806-9e67-4d07-bc34-a7e1bdb27b7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# we will write another function of y as z\n",
        "z = y*y + 1\n",
        "print(z)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[37., 37.],\n",
            "        [37., 37.],\n",
            "        [37., 37.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R_6iub2ReVEa",
        "colab_type": "code",
        "outputId": "35c1561f-36f3-421c-be83-73dae69615b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "t = torch.sum(z) \n",
        "print(t)\n",
        "\n",
        "# Here book-keeping is done and it spacified that this is last function as it its an inbuilt sum function of torch."
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(222., grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AEGC1Txze5Cn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Now you can see all the function starting from x to t as a forward pass in neural networks\n",
        "# Where each operation is performed at each layer.\n",
        "\n",
        "# Now lets do backward pass or backpropagation. \n",
        "# we can do this using backward() function\n",
        "\n",
        "t.backward()\n",
        "\n",
        "# at this time it shows nothing but; it perform gradient operation on back"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rXUIn0jYfz-7",
        "colab_type": "code",
        "outputId": "da278200-f439-40de-e37e-17f7f862f34f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# now we can see the results using x.grad means derivative of t w.r.t. x. \n",
        "# As z is a function of y and y is a function of x and hence z is a function of x\n",
        "\n",
        "print(x.grad)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[12., 12.],\n",
            "        [12., 12.],\n",
            "        [12., 12.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BigUlaFghYkg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$ t = \\sum_i z_i $\n",
        "\n",
        "$ z_i = y_i^2 + 1$\n",
        " \n",
        " $ y_i = x_i+5$\n",
        "\n",
        "$\\frac{\\partial t}{\\partial x_i} = \\frac{\\partial z_i}{\\partial x_i} = \\frac{\\partial z_i}{\\partial y_i} \\frac{\\partial y_i}{\\partial x_i} $\n",
        "\n",
        "$\\frac{\\partial z_i}{\\partial y_i} = 2y_i$  $and$ $  \\frac{\\partial y_i}{\\partial x_i} =1 $\n",
        "\n",
        "$ when$ $x_i = 1$ $ y_i = 6$ $ and $ $ \\frac{\\partial t}{\\partial x_i} = \\frac{\\partial z_i}{\\partial x_i} = 2y_i = 12$\n",
        "\n",
        "Thus we can see that function backward uses chain rule."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "44e8834d-f562-4230-f841-c9b95ac9df31",
        "id": "XG9lkcdrP1a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# lets see another example\n",
        "x = torch.ones(3, 2, requires_grad=True)\n",
        "\n",
        "y = x + 1\n",
        "\n",
        "z = 1/(1+torch.exp(-y)) # sigmoid function \n",
        "\n",
        "print(z)\n",
        "\n",
        "r = torch.sum(z)\n",
        "\n",
        "r.backward()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.8808, 0.8808],\n",
            "        [0.8808, 0.8808],\n",
            "        [0.8808, 0.8808]], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IOQc8Ds2th3H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$ r = \\sum_i z_i$\n",
        "\n",
        "$ z_i = \\frac{1}{1 + e^{-y_i}}$\n",
        "\n",
        "\n",
        "$ y_i = x_i + 1$\n",
        "\n",
        "$\\frac{\\partial r}{\\partial x_i} = \\frac{\\partial z_i}{\\partial x_i} = \\frac{\\partial z_i}{\\partial y_i} \\frac{\\partial y_i}{\\partial x_i} $\n",
        "\n",
        "$\\frac{\\partial z_i}{\\partial y_i} = z(1-z)$ \n",
        "[See the derivative of sigmoid here](http://www.ai.mit.edu/courses/6.892/lecture8-html/sld015.htm)\n",
        "\n",
        "$ \\frac{\\partial y_i}{\\partial x_i} = 1$\n",
        "\n",
        "$ \\frac{\\partial r}{\\partial x_i} = \\frac{\\partial z_i}{\\partial x_i} = \\frac{\\partial z_i}{\\partial y_i} = z(1-z) = 0.1050 $\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yYZ4FAJEv-Ku",
        "colab_type": "code",
        "outputId": "6533fb93-f471-4a41-bc8f-022f68f0c1ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print(x.grad)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1050, 0.1050],\n",
            "        [0.1050, 0.1050],\n",
            "        [0.1050, 0.1050]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pD_Yq2W4R4c_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Autograd Example"
      ]
    },
    {
      "metadata": {
        "id": "J7c89Ok61Hr7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = torch.randn([20, 1], requires_grad=True)\n",
        "\n",
        "y = 3*x - 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h6DcaxMTTXth",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w = torch.tensor([1.], requires_grad=True)\n",
        "b = torch.tensor([1.], requires_grad=True)\n",
        "\n",
        "y_hat = w*x + b  # a linear function\n",
        "\n",
        "loss = torch.sum((y_hat - y)**2) # loss function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iuvwQZG_T4QT",
        "colab_type": "code",
        "outputId": "9b3eaa53-bd5f-4ad0-c0af-3378c0262b87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(192.9307, grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5Pih2VujT6gV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z5BSBFu7T_w_",
        "colab_type": "code",
        "outputId": "3f539cfe-859d-4194-b9af-b60d8fde403a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(w.grad, b.grad) # as the gradient is -ve we need to increase the weights i.e. moving opposite to gradient"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-20.7903]) tensor([114.7603])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_G3q6fhFUVSe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1jkap32gWqAR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# In loops\n",
        "\n",
        "-> Write the above autograd algorithm in loop and make it a learning alogorithm"
      ]
    },
    {
      "metadata": {
        "id": "ukUqpMPvW2W1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "050bbf05-411c-4006-e246-a1199deaf785"
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "w = torch.tensor([1.], requires_grad=True)\n",
        "b = torch.tensor([1.], requires_grad=True)\n",
        "\n",
        "print(w.item(), b.item())\n",
        "\n",
        "for i in range(10):\n",
        "  \n",
        "  x = torch.randn(3, 2)\n",
        "  \n",
        "  y = 3*x -2 \n",
        "  \n",
        "  y_hat = w*x + b\n",
        "  \n",
        "  loss = torch.sum((y_hat - y)**2)\n",
        "  \n",
        "  loss.backward()\n",
        "  \n",
        "  '''\n",
        "    Now we have done forward pass as well as backward pass but; pytorch doesn't have any idea of it. \n",
        "    We need to updated the weight now but; pytorch still consider it as next equation in forward pass so it will take those update equations in gradient calculations which shouldn't be cosidered as per gradient descent algorithms.\n",
        "    Hence to avoid this mistake we will use torch.no_grad().\n",
        "  '''\n",
        "  \n",
        "  # updating weights\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "    b -= learning_rate * b.grad\n",
        "    \n",
        "    '''\n",
        "      we need to make gradients of w and b zero for refreshing them in next iterations\n",
        "    '''\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "    \n",
        "  print(w.item(), b.item()) # we get the final value of w and b in last iteration which will fit into equation."
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0 1.0\n",
            "1.3342381715774536 0.6081753969192505\n",
            "1.5825026035308838 0.24793660640716553\n",
            "1.660771369934082 0.04386614263057709\n",
            "1.8801357746124268 -0.2932453751564026\n",
            "1.9151567220687866 -0.4846447706222534\n",
            "2.1496729850769043 -0.7035585641860962\n",
            "2.180433511734009 -0.8516274690628052\n",
            "2.224364995956421 -1.0012438297271729\n",
            "2.2460215091705322 -1.107055902481079\n",
            "2.275125503540039 -1.1307240724563599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "93vngdgDo3Dd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# For large problems"
      ]
    },
    {
      "metadata": {
        "id": "Oyfe2nznXQle",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "adc44085-6ddd-447e-997e-3dbc1cde863b"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "N = 1000000\n",
        "\n",
        "epochs = 200\n",
        "\n",
        "w = torch.randn([N], requires_grad=True)\n",
        "b = torch.randn([1], requires_grad=True)\n",
        "\n",
        "#print(torch.mean(w).item(), b.item())\n",
        "\n",
        "x = torch.randn([N])\n",
        "\n",
        "#print(x.shape)\n",
        "\n",
        "\n",
        "for i in range(epochs):\n",
        "  \n",
        "  x = torch.randn([N])\n",
        "  \n",
        "  y = torch.dot(3*torch.ones([N]), x) - 2\n",
        "  \n",
        "  y_hat = torch.dot(w,x) + b\n",
        "  \n",
        "  loss = torch.sum((y_hat - y)**2)\n",
        "  \n",
        "  loss.backward()\n",
        "  \n",
        "  \n",
        "  # updating weights\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "    b -= learning_rate * b.grad\n",
        "    \n",
        "    \n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "    \n",
        "  \n",
        "\n",
        "print(\"Final Values of w and b: \", torch.mean(w).item(), b.item(), \"\\n\\n\") # we get the final value of w and b in last iteration which will fit into equation.\n"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Values of w and b:  nan nan \n",
            "\n",
            "\n",
            "CPU times: user 2.8 s, sys: 1.06 s, total: 3.87 s\n",
            "Wall time: 3.88 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5Zsqe6skqg45",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "fcf033e0-bff4-44db-9eb4-4ebf694b02ed"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "N = 1000000\n",
        "\n",
        "epochs = 200\n",
        "\n",
        "w = torch.randn([N], requires_grad=True, device=cuda0)\n",
        "b = torch.randn([1], requires_grad=True, device=cuda0)\n",
        "\n",
        "#print(torch.mean(w).item(), b.item())\n",
        "\n",
        "x = torch.randn([N])\n",
        "\n",
        "#print(x.shape)\n",
        "\n",
        "\n",
        "for i in range(epochs):\n",
        "  \n",
        "  x = torch.randn([N], device=cuda0)\n",
        "  \n",
        "  y = torch.dot(3*torch.ones([N], device=cuda0), x) - 2\n",
        "  \n",
        "  y_hat = torch.dot(w,x) + b\n",
        "  \n",
        "  loss = torch.sum((y_hat - y)**2)\n",
        "  \n",
        "  loss.backward()\n",
        "  \n",
        "  \n",
        "  # updating weights\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "    b -= learning_rate * b.grad\n",
        "    \n",
        "    \n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "    \n",
        "  \n",
        "\n",
        "print(\"Final Values of w and b: \", torch.mean(w).item(), b.item(), \"\\n\\n\") # we get the final value of w and b in last iteration which will fit into equation.\n"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Values of w and b:  nan nan \n",
            "\n",
            "\n",
            "CPU times: user 148 ms, sys: 47.5 ms, total: 196 ms\n",
            "Wall time: 202 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "51vb0pxKumxe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}